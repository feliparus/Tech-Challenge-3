{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6aed4b-a3c9-4d27-95f8-bc420baf5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.utils import *\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc5685-4712-4e49-9fa4-b952be7a3631",
   "metadata": {},
   "source": [
    "**Objetivo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9339964-b416-48a2-aa5f-b2b8abaad7e9",
   "metadata": {},
   "source": [
    "O objetivo deste script é processar e armazenar embeddings de textos de produtos, atualizar um índice FAISS para busca eficiente e garantir a continuidade do processamento caso ele seja interrompido. Vamos realizar o carregamento dos dados, calcular e salvar embeddings, e atualizar o índice FAISS com novos embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e9782-a613-4c04-8b1f-6457e5ec3f0c",
   "metadata": {},
   "source": [
    "**Configurações iniciais**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80996ae-ddbd-462b-b59a-1d4c2bc079fe",
   "metadata": {},
   "source": [
    "Definimos os caminhos para os arquivos de dados e de índices que serão utilizados durante o processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58d561-08b0-41e9-a263-1be2e28f4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio_arquivos = '../arquivos'\n",
    "parquet_file_path = os.path.join(diretorio_arquivos, 'trn.parquet')\n",
    "\n",
    "# Variáveis embeedings\n",
    "embeddings_file_path = os.path.join(diretorio_arquivos,'embeddings','embeddings.h5')\n",
    "faiss_index_path = os.path.join(diretorio_arquivos,'embeddings','amazon_products_index.faiss')\n",
    "processed_indices_embeedings_file_path = os.path.join(diretorio_arquivos,'embeddings','processed_indices.npy')\n",
    "\n",
    "# Variáveis finetunning\n",
    "diretorio_finetunning_modelo_final = os.path.join(diretorio_arquivos, 'finetunning','modelo_final')\n",
    "processed_indices_finetunning_file_path = os.path.join(diretorio_arquivos, 'finetunning', 'processed_indices.npy')\n",
    "\n",
    "# Carregar DataFrame do Parquet\n",
    "dados = load_dataframe(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c534c1-8eff-4484-94a4-cd3ee396c089",
   "metadata": {},
   "source": [
    "**Salvamento de Embeddings em Arquivo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c28312-eae5-4d93-9b43-8b7bef24fdee",
   "metadata": {},
   "source": [
    "Configuramos o preprocessamento para gerar embeddings usando o modelo DistilBERT, calculamos os embeddings a partir da última camada oculta e salvamos os resultados em um arquivo HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dc469-02b3-4bd5-bd35-74f9598cd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento para embeddings\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Usando o mesmo modelo que no fine-tuning (para classificação), mas pegando os embeddings\n",
    "model = DistilBertForSequenceClassification.from_pretrained(diretorio_finetunning_modelo_final, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067809d1-b912-4c0f-bf32-96f8ebb6b79e",
   "metadata": {},
   "source": [
    "**Configuração de Parâmetros e Preparação dos Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae0b6b-7fcf-4f00-9878-4623613ebfad",
   "metadata": {},
   "source": [
    "Determinamos o índice inicial para continuar o processamento dos embeddings, verificamos o checkpoint para evitar duplicações e ajustamos a lista de textos dependendo do uso de amostras ou de todo o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6648a06-7455-4c36-a970-316444844878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar o índice inicial para processamento\n",
    "start_idx_finetunning = np.load(processed_indices_finetunning_file_path).item()\n",
    "batch_size = 4                       # Número de amostras a serem processadas em cada execução\n",
    "use_sample = False                   # False processa todos os dados\n",
    "\n",
    "textos = [f\"Título: {title} Descrição: {content}\" for title, content in zip(dados['title'], dados['content'])]\n",
    "\n",
    "# Verificar o ponto de partida (checkpoint) de onde continuar\n",
    "start_idx = 0\n",
    "if os.path.exists(embeddings_file_path):\n",
    "    with h5py.File(embeddings_file_path, 'r') as f:\n",
    "        if 'embeddings' in f:\n",
    "            start_idx = f['embeddings'].shape[0]\n",
    "            print(f\"Continuando a partir do índice {start_idx}.\")\n",
    "else:\n",
    "    print(\"Arquivo de embeddings não encontrado. Iniciando do início.\")\n",
    "\n",
    "# Ajustando textos quando usado amostras\n",
    "if use_sample:\n",
    "    end_idx = min(start_idx + batch_size, start_idx_finetunning)  # Garante que não ultrapasse o limite\n",
    "    textos = textos[start_idx:end_idx]\n",
    "    print(f\"Usando uma amostra de {len(textos)} registros.\")\n",
    "else:\n",
    "    textos = textos[start_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8e3fa-50c4-431a-b4b5-bd7d10ac3ad5",
   "metadata": {},
   "source": [
    "**Processamento de Textos e Cálculo de Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4896d2-f328-4b2a-818c-953dbe501c1e",
   "metadata": {},
   "source": [
    "Geramos embeddings em lotes e salvamos incrementalmente, atualizando o progresso a cada lote processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9d905-28a1-462a-82a5-21bb06ef7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeddings = None\n",
    "\n",
    "# Gerar embeddings em lotes e salvar incrementalmente\n",
    "for i in tqdm(range(start_idx, len(textos), batch_size), desc='Calculando embeddings'):\n",
    "    batch_textos = textos[i:i + batch_size]\n",
    "    batch_embeddings = generate_embeddings(batch_textos, tokenizer, model)\n",
    "    save_embeddings_to_file(batch_embeddings, embeddings_file_path)\n",
    "\n",
    "    # Atualizar o progresso\n",
    "    tqdm.write(f'Lote processado: {i // batch_size + 1}/{len(textos) // batch_size + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3fa3a-7763-406f-9f7c-3fa112663ab7",
   "metadata": {},
   "source": [
    "**Atualização do Índice FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73c5dd-29a9-40af-9fb8-82a28d79e6be",
   "metadata": {},
   "source": [
    "Atualizamos e salvamos o índice FAISS com os embeddings carregados, criando o índice se necessário e adicionando novos embeddings ao existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3fdca-c1f3-4691-81c8-763e41fc134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar e salvar o índice FAISS\n",
    "if os.path.exists(embeddings_file_path):\n",
    "    with h5py.File(embeddings_file_path, 'r') as f:\n",
    "        if 'embeddings' in f:\n",
    "            # Verifica a dimensão dos embeddings a partir do arquivo\n",
    "            batch_embeddings_shape = f['embeddings'].shape[1]\n",
    "            index = faiss.IndexFlatL2(batch_embeddings_shape)\n",
    "            print(f\"Índice FAISS criado com dimensão {batch_embeddings_shape}.\")\n",
    "            \n",
    "            if os.path.exists(faiss_index_path):\n",
    "                index = faiss.read_index(faiss_index_path)\n",
    "                print(f\"Carregado índice FAISS existente com {index.ntotal} embeddings.\")\n",
    "            \n",
    "            embeddings_to_add = f['embeddings'][start_idx:]\n",
    "            if embeddings_to_add.shape[0] > 0:\n",
    "                index.add(embeddings_to_add)\n",
    "                print(f\"Adicionados {embeddings_to_add.shape[0]} novos embeddings ao índice FAISS.\")\n",
    "            \n",
    "            faiss.write_index(index, faiss_index_path)\n",
    "            print(f\"Índice FAISS atualizado salvo em '{faiss_index_path}'.\")\n",
    "        else:\n",
    "            print(\"O dataset 'embeddings' não foi encontrado no arquivo.\")\n",
    "else:\n",
    "    print(\"Nenhum embedding gerado para atualizar o índice FAISS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
