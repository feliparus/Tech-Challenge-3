{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3c9455-4023-4a9d-91be-8c25ed99e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from include.utils import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5210d13-f08c-4fdf-9a75-898ac9b9a3ed",
   "metadata": {},
   "source": [
    "**Configurações iniciais**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eceadcf-32fb-4883-9030-c2b1566e6f48",
   "metadata": {},
   "source": [
    "Definimos os caminhos para os arquivos de dados e índice FAISS e carregamos o DataFrame do arquivo Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a761bdd5-4150-4c5e-b273-cb87c1010919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "diretorio_arquivos = '../arquivos'\n",
    "parquet_file_path = os.path.join(diretorio_arquivos, 'trn.parquet')\n",
    "diretorio_finetunning_modelo_final = os.path.join(diretorio_arquivos, 'finetunning','modelo_final')\n",
    "faiss_index_path = os.path.join(diretorio_arquivos,'embeddings','amazon_products_index.faiss')\n",
    "\n",
    "# Carregar DataFrame do Parquet\n",
    "dados = load_dataframe(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84326fc-72b9-4486-b704-97d091539911",
   "metadata": {},
   "source": [
    "**Configuração e Carregamento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05831e3a-c881-4ddf-9df6-70abda083863",
   "metadata": {},
   "source": [
    "Configuramos o tradutor, o modelo SentenceTransformer e carregamos o modelo e tokenizer finetuned, bem como o índice FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d02ea7-730c-4feb-9e59-8df79868d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo finetuned de ../arquivos\\finetunning\\modelo_final...\n",
      "Carregando índice FAISS de ../arquivos\\embeddings\\amazon_products_index.faiss...\n",
      "Índice FAISS carregado com 100 vetores.\n"
     ]
    }
   ],
   "source": [
    "# Configuração do tradutor\n",
    "translator = GoogleTranslator(source='en', target='pt')\n",
    "\n",
    "# Carregamento do modelo e tokenizer finetuned\n",
    "print(f\"Carregando modelo finetuned de {diretorio_finetunning_modelo_final}...\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(diretorio_finetunning_modelo_final)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(diretorio_finetunning_modelo_final)\n",
    "\n",
    "# Carregamento do índice FAISS\n",
    "print(f\"Carregando índice FAISS de {faiss_index_path}...\")\n",
    "faiss_index = faiss.read_index(faiss_index_path)\n",
    "print(f\"Índice FAISS carregado com {faiss_index.ntotal} vetores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249cd5d-14f1-4b5f-b961-cf1ab10df116",
   "metadata": {},
   "source": [
    "**Processamento de Perguntas e Respostas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de40723-d2d5-449f-9797-451b08baa1ba",
   "metadata": {},
   "source": [
    "Configuramos a função para dividir o contexto em blocos menores, classificar a relevância dos blocos baseados na pergunta, traduzir o texto, encontrar o bloco mais relevante e identificar os livros mais relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbd7c1e-9aa5-4aae-870a-1cc2e029c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para dividir o contexto em blocos menores\n",
    "def split_context_into_blocks(context, block_size=100):\n",
    "    words = context.split()\n",
    "    return [' '.join(words[i:i + block_size]) for i in range(0, len(words), block_size)]\n",
    "\n",
    "\n",
    "# Função para classificar a relevância dos blocos de um contexto baseado na pergunta\n",
    "def classify_block_relevance(question, context, block_size=100):\n",
    "    relevance_scores = []\n",
    "    blocks = split_context_into_blocks(context, block_size)\n",
    "\n",
    "    for block in blocks:\n",
    "        inputs = tokenizer.encode_plus(question, block, return_tensors='pt', max_length=512, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Supondo que temos uma tarefa de classificação binária, pegamos a pontuação da classe positiva\n",
    "        logits = outputs.logits\n",
    "        score = torch.softmax(logits, dim=1).max(dim=1).values.item()  # Pegamos a probabilidade da classe mais provável\n",
    "        relevance_scores.append((block, score))\n",
    "\n",
    "    # Ordena os blocos com base na pontuação de relevância (do mais alto para o mais baixo)\n",
    "    relevance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevance_scores\n",
    "    \n",
    "\n",
    "# Função para traduzir o texto (por exemplo, usando um tradutor fictício)\n",
    "def traduzir_texto(texto):\n",
    "    try:\n",
    "        return translator.translate(texto)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na tradução: {e}\")\n",
    "        return texto\n",
    "        \n",
    "\n",
    "# Função para encontrar o bloco mais relevante e responder à pergunta\n",
    "def find_most_relevant_block(question, context):\n",
    "    relevance_scores = classify_block_relevance(question, context)\n",
    "    \n",
    "    if not relevance_scores:\n",
    "        return \"Nenhum bloco relevante encontrado.\"\n",
    "\n",
    "    # O bloco mais relevante é o primeiro na lista após a ordenação\n",
    "    best_block, best_score = relevance_scores[0]\n",
    "    best_block_pt = traduzir_texto(best_block)  # Traduzir o bloco para o português\n",
    "    return f\"Bloco mais relevante (pontuação: {round(best_score, 2)}):\\n{best_block_pt}\"\n",
    "\n",
    "def find_top_relevant_books(question, dataframe, top_n=3):\n",
    "    all_relevance_scores = []\n",
    "\n",
    "    # Use tqdm para adicionar uma barra de progresso\n",
    "    for _, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0], desc=\"Processando livros\"):\n",
    "        titulo = row['title']\n",
    "        contexto = row['content']\n",
    "        relevance_scores = classify_block_relevance(question, contexto)\n",
    "        if relevance_scores:\n",
    "            best_block, best_score = relevance_scores[0]  # Melhor bloco\n",
    "            all_relevance_scores.append((titulo, best_block, best_score))\n",
    "\n",
    "    # Ordena todos os livros com base na pontuação de relevância (do mais alto para o mais baixo)\n",
    "    all_relevance_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    top_books = all_relevance_scores[:top_n]\n",
    "\n",
    "    return top_books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcaefb-e46b-46bb-b684-7197bbb17952",
   "metadata": {},
   "source": [
    "**Teste e Exibição de Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db41e9f-46ae-4929-8c87-b8f23d1c6e5f",
   "metadata": {},
   "source": [
    "Usamos uma pergunta específica para testar o código com uma pequena amostra de dados, encontrando o bloco mais relevante e traduzindo a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906b2b1d-abce-4774-9ddc-0ee83e20da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Título: Girls Ballet Tutu Neon Pink ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\luiz.santos\\AppData\\Local\\Temp\\ipykernel_20804\\142051507.py:12: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  print(f\"\\Contexto (us): {contexto}\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Contexto (us): High quality 3 layer ballet tutu. 12 inches in length\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.5):\n",
      "Tutu de balé de 3 camadas de alta qualidade. 12 polegadas de comprimento\n",
      "\n",
      "\n",
      "--- Título: Mog's Kittens ---\n",
      "\\Contexto (us): Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.5):\n",
      "As aventuras mais vendidas de Judith Kerr daquele gato cativante (e exasperante) Mog têm entretido crianças por mais de 30 anos. Agora, até mesmo bebês e crianças pequenas podem se divertir conhecendo esse adorável felino. Esses pequenos livros cartonados resistentes, com suas imagens brilhantes e simples, texto fácil e formatos amigáveis ​​às mãos, são perfeitos para encantar os mais jovens. Idades de 6 meses a 2 anos.\n",
      "\n",
      "\n",
      "--- Título: Girls Ballet Tutu Neon Blue ---\n",
      "\\Contexto (us): Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Tutu de dança para meninas de 2 a 8 anos. Perfeito para prática de dança, recitais e performances, fantasias ou apenas para diversão!\n",
      "\n",
      "\n",
      "--- Título: The Prophet ---\n",
      "\\Contexto (us): In a distant, timeless place, a mysterious prophet walks the sands. At the moment of his departure, he wishes to offer the people gifts but possesses nothing. The people gather round, each asks a question of the heart, and the man's wisdom is his gift. It is Gibran's gift to us, as well, for Gibran's prophet is rivaled in his wisdom only by the founders of the world's great religions. On the most basic topics--marriage, children, friendship, work, pleasure--his words have a power and lucidity that in another era would surely have provoked the description \"divinely inspired.\" Free of dogma, free of power structures and metaphysics, consider these poetic, moving aphorisms a 20th-century supplement to all sacred traditions--as millions of other readers already have.--Brian Bruya--This text refers to theHardcoveredition.\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Em um lugar distante e atemporal, um profeta misterioso caminha pelas areias. No momento de sua partida, ele deseja oferecer presentes ao povo, mas não possui nada. O povo se reúne ao redor, cada um faz uma pergunta ao coração, e a sabedoria do homem é seu presente. É o presente de Gibran para nós também, pois o profeta de Gibran é rivalizado em sua sabedoria apenas pelos fundadores das grandes religiões do mundo. Sobre os tópicos mais básicos — casamento, filhos, amizade, trabalho, prazer — suas palavras têm um poder e uma lucidez que em outra era certamente teriam provocado a descrição \"divinamente inspirado\". Livre de dogmas,\n",
      "\n",
      "\n",
      "--- Título: Rightly Dividing the Word ---\n",
      "\\Contexto (us): --This text refers to thePaperbackedition.\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "--Este texto se refere à edição de bolso.\n",
      "\n",
      "\n",
      "--- Título: Worship with Don Moen [VHS] ---\n",
      "\\Contexto (us): Worship with Don Moen [VHS]\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Adoração com Don Moen [VHS]\n",
      "\n",
      "\n",
      "--- Título: Autumn Story Brambly Hedge ---\n",
      "\\Contexto (us): \"!the most research-crammed fantasy ever set before small children!\" Sunday Times Magazine\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.5):\n",
      "\"!a fantasia mais cheia de pesquisa já apresentada a crianças pequenas!\" Revista Sunday Times\n",
      "\n",
      "\n",
      "--- Título: Spirit Led-Moving By Grace In The Holy Spirit's Gifts ---\n",
      "\\Contexto (us): You can flow effortlessly and powerfully in the gifts of the Holy Spirit today! Join Joseph Prince as he expounds on the nine gifts of the Holy Spirit and be encouraged to know that every believer-including you-can operate in these grace gifts. Understand how the Spirit leads you in using these gifts and why ministering in love is so important. Discover the secret to stepping out in faith and love, and moving super-naturally in the Spirit. Find out also what your gifts are and how to flow in them. Whether it's in the area of your family, ministry or work, the messages in this powerful DVD resource will stir your faith and help you to operate and minister in the power of the Holy Spirit! 6-DVD Set(7 Sermons-approx. total duration: 5hr 47min)\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Você pode fluir sem esforço e poderosamente nos dons do Espírito Santo hoje! Junte-se a Joseph Prince enquanto ele expõe os nove dons do Espírito Santo e seja encorajado a saber que todo crente - incluindo você - pode operar nesses dons da graça. Entenda como o Espírito o conduz no uso desses dons e por que ministrar em amor é tão importante. Descubra o segredo para dar um passo na fé e no amor, e se mover sobrenaturalmente no Espírito. Descubra também quais são seus dons e como fluir neles. Seja na área de sua família, ministério ou trabalho,\n",
      "\n",
      "\n",
      "--- Título: The Very Bad Bunny (Beginner Series) ---\n",
      "\\Contexto (us): By Marilyn Sadler, Illustrated by Roger Bollen\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Por Marilyn Sadler, ilustrado por Roger Bollen\n",
      "\n",
      "\n",
      "--- Título: Nice for Mice ---\n",
      "\\Contexto (us): Jill Barklem was born in Epping in 1951. After an accident when she was thirteen, Jill was unable to take part in PE or games at school and instead developed her talent for drawing and art. On leaving school, she studied illustration at St Martin's in London. Jill is now a full-time illustrator, working on the series of Brambly Hedge books.\n",
      "\n",
      "Resposta (pt): Bloco mais relevante (pontuação: 0.51):\n",
      "Jill Barklem nasceu em Epping em 1951. Após um acidente quando tinha treze anos, Jill não conseguiu participar de educação física ou jogos na escola e, em vez disso, desenvolveu seu talento para desenho e arte. Ao deixar a escola, ela estudou ilustração na St Martin's em Londres. Jill agora é uma ilustradora em tempo integral, trabalhando na série de livros Brambly Hedge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perguntas específicas\n",
    "pergunta = \"Quais são as principais características deste produto?\"\n",
    "\n",
    "# Exemplo de uso com dados do DataFrame\n",
    "for _, row in dados.head(10).iterrows():  # Limitar a quantidade para teste\n",
    "    titulo = row['title']\n",
    "    contexto = row['content']\n",
    "    \n",
    "    print(f\"\\n--- Título: {titulo} ---\")\n",
    "    \n",
    "    resposta = find_most_relevant_block(pergunta, contexto)\n",
    "    print(f\"\\Contexto (us): {contexto}\\n\")\n",
    "    print(f\"Resposta (pt): {resposta}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90d1bd-6d3b-4c19-bf7e-75bd3930284a",
   "metadata": {},
   "source": [
    "Configuramos a pergunta para retornar os três melhores livros, processamos o DataFrame para encontrar os mais relevantes e exibimos os resultados com a descrição traduzida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95b9bee-0ca9-47b4-b827-a30bca711e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando livros:   1%|▊                                                                              | 16591/1498718 [47:38<70:56:32,  5.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m pergunta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoderia me retornar os três melhores livros ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Exemplo de uso com dados do DataFrame\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m top_books \u001b[38;5;241m=\u001b[39m find_top_relevant_books(pergunta, dados)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Exibir os melhores livros\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (titulo, bloco, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_books, \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m, in \u001b[0;36mfind_top_relevant_books\u001b[1;34m(question, dataframe, top_n)\u001b[0m\n\u001b[0;32m     54\u001b[0m titulo \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     55\u001b[0m contexto \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m relevance_scores \u001b[38;5;241m=\u001b[39m classify_block_relevance(question, contexto)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevance_scores:\n\u001b[0;32m     58\u001b[0m     best_block, best_score \u001b[38;5;241m=\u001b[39m relevance_scores[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Melhor bloco\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mclassify_block_relevance\u001b[1;34m(question, context, block_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(question, block, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Supondo que temos uma tarefa de classificação binária, pegamos a pontuação da classe positiva\u001b[39;00m\n\u001b[0;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:883\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    881\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 883\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    884\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    885\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    886\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    887\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    888\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    889\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    890\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    891\u001b[0m )\n\u001b[0;32m    892\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    893\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:703\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    704\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    705\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    706\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    707\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    708\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    709\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    710\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:464\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    456\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    457\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    458\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m         output_attentions,\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    465\u001b[0m         hidden_state,\n\u001b[0;32m    466\u001b[0m         attn_mask,\n\u001b[0;32m    467\u001b[0m         head_mask[i],\n\u001b[0;32m    468\u001b[0m         output_attentions,\n\u001b[0;32m    469\u001b[0m     )\n\u001b[0;32m    471\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:390\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    391\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    392\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    393\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    394\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    395\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    396\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    397\u001b[0m )\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    399\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:208\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m dim_per_head)\n\u001b[0;32m    207\u001b[0m q \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_lin(query))  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m k \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    209\u001b[0m v \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(dim_per_head)  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pergunta = \"Poderia me retornar os três melhores livros ?\"\n",
    "\n",
    "# Exemplo de uso com dados do DataFrame\n",
    "top_books = find_top_relevant_books(pergunta, dados)\n",
    "\n",
    "# Exibir os melhores livros\n",
    "for i, (titulo, bloco, score) in enumerate(top_books, 1):\n",
    "    bloco_pt = traduzir_texto(bloco)\n",
    "    print(f\"\\n--- Livro {i}: {titulo} ---\")\n",
    "    print(f\"Relevância: {round(score, 2)}\")\n",
    "    print(f\"Descrição traduzida: {bloco_pt}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
