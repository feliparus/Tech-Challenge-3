{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0810b7ee-c11e-4611-b8d4-f488e8028af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3358933-1204-4906-ae23-955f248cd9a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 1073741824 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m parquet_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(diretorio_arquivos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrn.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Carregar DataFrame do Parquet\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m dados \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(parquet_file_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Configurações de amostragem\u001b[39;00m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mread(columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39muse_threads,\n\u001b[0;32m   1844\u001b[0m                     use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   1486\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_expression,\n\u001b[0;32m   1487\u001b[0m     use_threads\u001b[38;5;241m=\u001b[39muse_threads\n\u001b[0;32m   1488\u001b[0m )\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\_dataset.pyx:562\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\_dataset.pyx:3804\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 1073741824 failed"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "diretorio_arquivos = '../arquivos'\n",
    "embeddings_file_path = os.path.join(diretorio_arquivos, 'embeddings.h5')\n",
    "faiss_index_path = os.path.join(diretorio_arquivos, 'amazon_products_index.faiss')\n",
    "parquet_file_path = os.path.join(diretorio_arquivos, 'trn.parquet')\n",
    "\n",
    "# Carregar DataFrame do Parquet\n",
    "dados = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Configurações de amostragem\n",
    "batch_size = 100\n",
    "use_sample = True\n",
    "\n",
    "# Amostrar dados se necessário\n",
    "if use_sample:\n",
    "    dados = dados.sample(n=batch_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0b81a-eb3b-4417-af2d-fe713c6fc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o DataFrame com os dados fornecidos\n",
    "\"\"\"\n",
    "data = {\n",
    "    \"uid\": [\"A1B2C3\", \"D4E5F6\", \"G7H8I9\", \"J1K2L3\", \"M4N5O6\", \"P7Q8R9\", \"S1T2U3\", \"V4W5X6\", \"Y7Z8A9\", \"B1C2D3\"],\n",
    "    \"title\": [\n",
    "        \"Deluxe Coffee Maker\", \"Smart Home Thermostat\", \"Ergonomic Office Chair\",\n",
    "        \"Portable Air Purifier\", \"High-Definition Smart TV\", \"Multi-Function Kitchen Blender\",\n",
    "        \"Wireless Noise-Canceling Headphones\", \"Leather Executive Briefcase\",\n",
    "        \"Outdoor Adventure Tent\", \"Digital Instant Camera\"\n",
    "    ],\n",
    "    \"content\": [\n",
    "        \"The Deluxe Coffee Maker is the ultimate kitchen appliance for coffee enthusiasts...\",\n",
    "        \"The Smart Home Thermostat offers advanced climate control with cutting-edge technology...\",\n",
    "        \"Experience unparalleled comfort with the Ergonomic Office Chair...\",\n",
    "        \"The Portable Air Purifier is perfect for improving air quality in any room...\",\n",
    "        \"Enjoy cinematic-quality visuals with the High-Definition Smart TV...\",\n",
    "        \"The Multi-Function Kitchen Blender is a versatile tool that handles everything from smoothies to soups...\",\n",
    "        \"Immerse yourself in high-fidelity sound with the Wireless Noise-Canceling Headphones...\",\n",
    "        \"The Leather Executive Briefcase combines elegance and practicality for the modern professional...\",\n",
    "        \"The Outdoor Adventure Tent is designed for explorers who seek comfort and durability in the wild...\",\n",
    "        \"Capture memories instantly with the Digital Instant Camera...\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dados = pd.DataFrame(data)\n",
    "\"\"\"\n",
    "\n",
    "# Carregar o modelo de embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Criar embeddings dos conteúdos\n",
    "embeddings = model.encode(dados['content'].tolist(), convert_to_tensor=True)\n",
    "normalized_embeddings = normalize(embeddings.numpy())\n",
    "\n",
    "# Criar o índice de busca\n",
    "dimension = normalized_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(normalized_embeddings)\n",
    "\n",
    "def retrieve_and_generate(query, top_k=1, distance_threshold=1.2):\n",
    "    # Codificar e normalizar a consulta\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    query_embedding = normalize(query_embedding.numpy())\n",
    "    \n",
    "    # Buscar no índice\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Ajustar o limite para distâncias\n",
    "    print(f\"Distâncias: {distances}\")\n",
    "    print(f\"Índices: {indices}\")\n",
    "    \n",
    "    if distances[0][0] < distance_threshold:  # Ajuste conforme necessário\n",
    "        # Recuperar o conteúdo mais relevante\n",
    "        relevant_content = dados.iloc[indices[0][0]]['content']\n",
    "        return relevant_content\n",
    "    else:\n",
    "        return \"Nenhuma resposta encontrada.\"\n",
    "\n",
    "def validate_answer(query, expected_answer, top_k=1, distance_threshold=1.2):\n",
    "    result = retrieve_and_generate(query, top_k, distance_threshold)\n",
    "    if result == expected_answer:\n",
    "        return f\"Resposta correta: {result}\"\n",
    "    else:\n",
    "        return f\"Resposta incorreta. Esperado: {expected_answer}. Obtido: {result}\"\n",
    "\n",
    "# Perguntas e Respostas Esperadas\n",
    "print(\"Perguntas em português:\")\n",
    "portuguese_queries = [\n",
    "    (\"O que faz o purificador de ar portátil?\", \"The Portable Air Purifier is perfect for improving air quality in any room...\"),\n",
    "    (\"Qual é a descrição do purificador de ar portátil?\", \"The Portable Air Purifier is perfect for improving air quality in any room...\"),\n",
    "    (\"Qual é a função do liquidificador multifuncional?\", \"The Multi-Function Kitchen Blender is a versatile tool that handles everything from smoothies to soups...\")\n",
    "]\n",
    "\n",
    "for query, expected_answer in portuguese_queries:\n",
    "    print(f\"Pergunta: {query}\")\n",
    "    print(validate_answer(query, expected_answer))\n",
    "    print()\n",
    "\n",
    "print(\"Questions in English:\")\n",
    "english_queries = [\n",
    "    (\"What is the function of the portable air purifier?\", \"The Portable Air Purifier is perfect for improving air quality in any room...\"),\n",
    "    (\"Describe the ergonomic office chair.\", \"Experience unparalleled comfort with the Ergonomic Office Chair...\"),\n",
    "    (\"How does the multi-function kitchen blender work?\", \"The Multi-Function Kitchen Blender is a versatile tool that handles everything from smoothies to soups...\"),\n",
    "    (\"What does the smart home thermostat do?\", \"The Smart Home Thermostat offers advanced climate control with cutting-edge technology...\"),\n",
    "    (\"Explain the features of the high-definition smart TV.\", \"Enjoy cinematic-quality visuals with the High-Definition Smart TV...\")\n",
    "]\n",
    "\n",
    "for query, expected_answer in english_queries:\n",
    "    print(f\"Question: {query}\")\n",
    "    print(validate_answer(query, expected_answer))\n",
    "    print()\n",
    "\n",
    "print(\"Testing with queries that should return 'No answer found':\")\n",
    "test_queries_no_answer = [\n",
    "    (\"What is the best way to use a coffee maker?\", \"Nenhuma resposta encontrada.\"),\n",
    "    (\"How do I set up a home theater system?\", \"Nenhuma resposta encontrada.\"),\n",
    "    (\"Describe the warranty on the digital camera.\", \"Nenhuma resposta encontrada.\"),\n",
    "    (\"What is the price of the leather briefcase?\", \"Nenhuma resposta encontrada.\"),\n",
    "    (\"Explain how the outdoor tent is set up.\", \"Nenhuma resposta encontrada.\")\n",
    "]\n",
    "\n",
    "for query, expected_answer in test_queries_no_answer:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(validate_answer(query, expected_answer))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb2e1fe-247e-492a-b01a-d43d592213a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de758a82cdd41eb97f3dfefbbd43c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luiz.santos\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-j-6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09be6e0f64de4c74ae2d9ea549632bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cc06ce6b0042a3a9c995625ffbf67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223532d833484d0bac36b7a37cb0b8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931f2e7a398e49c59f0ee74be02e7d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050f9b2530a946e1832ebc0b9832829e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807f214a8cc947929d20412609e4981d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.santos\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 24207.82 MB. The target location C:\\Users\\luiz.santos\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-j-6B\\blobs only has 17184.48 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d160bb7de54ce2a4d82d94a3ed898f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Carregar o modelo GPT-J\u001b[39;00m\n\u001b[0;32m     57\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/gpt-j-6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m model_gptj \u001b[38;5;241m=\u001b[39m GPTJForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/gpt-j-6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Função para gerar a resposta\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgerar_resposta\u001b[39m(pergunta, contextos):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:3579\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3577\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   3578\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 3579\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3580\u001b[0m             pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs\n\u001b[0;32m   3581\u001b[0m         )\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3585\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3586\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   3587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   3588\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    403\u001b[0m         path_or_repo_id,\n\u001b[0;32m    404\u001b[0m         filename,\n\u001b[0;32m    405\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    406\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    407\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    408\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    409\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    410\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    411\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    412\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    413\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    414\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1241\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1243\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1245\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1246\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1247\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1250\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1251\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1252\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1253\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1256\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1257\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1389\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1387\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1389\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1390\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1391\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1392\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1393\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1394\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1395\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1396\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1397\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1398\u001b[0m     )\n\u001b[0;32m   1399\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1915\u001b[0m     http_get(\n\u001b[0;32m   1916\u001b[0m         url_to_download,\n\u001b[0;32m   1917\u001b[0m         f,\n\u001b[0;32m   1918\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1919\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1920\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1921\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1922\u001b[0m     )\n\u001b[0;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:549\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    547\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 549\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    551\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPTJForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Criar o DataFrame com os dados fornecidos\n",
    "data = {\n",
    "    \"uid\": [\"A1B2C3\", \"D4E5F6\", \"G7H8I9\", \"J1K2L3\", \"M4N5O6\", \"P7Q8R9\", \"S1T2U3\", \"V4W5X6\", \"Y7Z8A9\", \"B1C2D3\"],\n",
    "    \"title\": [\n",
    "        \"Deluxe Coffee Maker\", \"Smart Home Thermostat\", \"Ergonomic Office Chair\",\n",
    "        \"Portable Air Purifier\", \"High-Definition Smart TV\", \"Multi-Function Kitchen Blender\",\n",
    "        \"Wireless Noise-Canceling Headphones\", \"Leather Executive Briefcase\",\n",
    "        \"Outdoor Adventure Tent\", \"Digital Instant Camera\"\n",
    "    ],\n",
    "    \"content\": [\n",
    "        \"The Deluxe Coffee Maker is the ultimate kitchen appliance for coffee enthusiasts...\",\n",
    "        \"The Smart Home Thermostat offers advanced climate control with cutting-edge technology...\",\n",
    "        \"Experience unparalleled comfort with the Ergonomic Office Chair...\",\n",
    "        \"The Portable Air Purifier is perfect for improving air quality in any room...\",\n",
    "        \"Enjoy cinematic-quality visuals with the High-Definition Smart TV...\",\n",
    "        \"The Multi-Function Kitchen Blender is a versatile tool that handles everything from smoothies to soups...\",\n",
    "        \"Immerse yourself in high-fidelity sound with the Wireless Noise-Canceling Headphones...\",\n",
    "        \"The Leather Executive Briefcase combines elegance and practicality for the modern professional...\",\n",
    "        \"The Outdoor Adventure Tent is designed for explorers who seek comfort and durability in the wild...\",\n",
    "        \"Capture memories instantly with the Digital Instant Camera...\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dados = pd.DataFrame(data)\n",
    "\n",
    "# Carregar o modelo de embeddings\n",
    "model_embedding = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Criar embeddings dos conteúdos\n",
    "embeddings = model_embedding.encode(dados['content'].tolist(), convert_to_tensor=True)\n",
    "normalized_embeddings = normalize(embeddings.numpy())\n",
    "\n",
    "# Criar o índice de busca\n",
    "dimension = normalized_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(normalized_embeddings)\n",
    "\n",
    "# Função para buscar os documentos mais relevantes\n",
    "def buscar_contexto(pergunta, model, index, dados):\n",
    "    pergunta_embedding = model.encode([pergunta], convert_to_tensor=True)\n",
    "    pergunta_embedding = normalize(pergunta_embedding.numpy())\n",
    "    \n",
    "    # Buscar os documentos mais relevantes\n",
    "    D, I = index.search(pergunta_embedding, k=3)  # k é o número de documentos a recuperar\n",
    "    documentos_relevantes = [dados.iloc[i]['content'] for i in I[0]]\n",
    "    \n",
    "    return documentos_relevantes\n",
    "\n",
    "# Carregar o modelo GPT-J\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model_gptj = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Função para gerar a resposta\n",
    "def gerar_resposta(pergunta, contextos):\n",
    "    contextos_completos = \" \".join(contextos)\n",
    "    prompt = f\"Pergunta: {pergunta}\\n\\nContexto:\\n{contextos_completos}\\n\\nResposta:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model_gptj.generate(inputs['input_ids'], max_length=150, num_return_sequences=1)\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return resposta\n",
    "\n",
    "# Exemplo de pergunta\n",
    "pergunta = \"Qual é o melhor produto para melhorar a qualidade do ar?\"\n",
    "\n",
    "# Buscar os documentos relevantes\n",
    "documentos_relevantes = buscar_contexto(pergunta, model_embedding, index, dados)\n",
    "\n",
    "# Gerar e imprimir a resposta\n",
    "resposta = gerar_resposta(pergunta, documentos_relevantes)\n",
    "print(\"Resposta:\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44370f3-1196-46ce-86af-1b765b7b3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
